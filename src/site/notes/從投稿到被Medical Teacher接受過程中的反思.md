---
{"dg-publish":true,"permalink":"/從投稿到被Medical Teacher接受過程中的反思/","title":"從投稿到被Medical Teacher接受過程中的反思","tags":["guideline","manuscript","ai","chatgpt"],"created":"2025-07-04T14:01","updated":"2025-07-04T20:02"}
---


近期一篇研究終於被Medical Teacher接受了，紀錄一下稿件準備到被接受的過程中的反思與心得。

這一篇研究標題為「Development and validation of a GPT-based Rater for Assessing Communication Skills Using the Gap-Kalamazoo Communication Skills Assessment Form」，我們利用ChatGPT的My GPTs服務建立GPT rater，這個GPT rater使用Gap-Kalamazoo Communication Skills Assessment Form評估醫療專業人員和病人互動過程中的溝通技巧。這個研究的主旨為發展並驗證這個GPT rater的評分信度以及效度。

### 稿件撰寫


> [!NOTE] 心得要點
> - 要熟悉研究指標的意義與使用限制，當多個指標的結果不一致時才知如何選擇與綜合結果。
> - 結果的解釋需全面。可以有所著重，但不能忽視缺點或限制。
> - 對數據結果的解讀定位要明確，才能給出明確清楚的研究結論。


這個研究我們比較了GPT rater和三組真人專家之間的評分者間信度，並利用真人專家評分結果的平均分數作為校標，計算校度。信度以及效度的指標包含了分數差異大小（mean absolute error%, MAE%）、Intraclass correlation coefficient (ICC)以及Pearson's r。我們的結果發現GPT rater和真人專家的評分結果差異不大（MAE% < 30%），但ICC和r的值都不高（< 0.40）。

原來我認為GPT rater的評分表現不佳，因為雖然MAE%結果顯示GPT和真人評分平均分數差不多，但ICC和r的結果顯示GPT和真人專家的評分並不一致。這結果似乎顯示，GPT rater的表現不佳。如果是這樣子的話，這一篇大概沒有機會被Medical Teacher所接受。

除了前述的結果外，我們同時也發現真人專家評分者之間的ICC、r和GPT跟真人間的ICC、r差不多。更重要的是，真人專家評分者、GPT rater的評分結果分佈範圍不大，顯示被評分者溝通能力相近，而這樣子的資料分佈可能限制了ICC、r的數值大小。在考量資料分佈造成的限制後，ICC、r這兩個指標的結果在我們的資料中較不適合作為可靠的研究指標。而基於MAE%，GPT rater的表現就沒那麼差，因為它的評分結果和真人專家相當接近。

然而，若僅依據MAE%的結果就直接斷定GPT rater的評分表現可接受，則顯得過於樂觀，也忽略了ICC和Pearson's r數值偏低可能代表的潛在限制與風險。儘管ICC和r的數值偏低可能源於資料分佈範圍較窄，但仍不能完全排除GPT rater本身表現不佳的可能性。因此，在撰寫結論時，要先清楚界定研究結果的適用範圍與限制，衡量各項證據的實際強度，避免對結果過度詮釋或過於保守。只有在精確把握研究指標的意義、完整呈現數據解讀的全貌後，才能做出客觀且具說服力的結論。


### 回應審查者

> [!NOTE] 心得要點
> - 要掌握審查者主要的問題點，針對問題回應。
> - 對審查者已知或已接受的論點不需再重複，點到即可。
> - 若要說服審查者，除了理論、概念上的推導，若能引述實徵結果佐證，更具說服力。
> - 和共同作者若對同一個問題的回應往來調整多次，一定有一些尚未獲得共識之處。可能還不知道問題徵結在哪，需要討論釐清。



### 使用大型語言模型潤飾稿件

> [!NOTE] 心得要點
> - 大型語言模型在協助段落、語句的連接以及轉折有非常強大的文字使用能力。
> - 大型語言模型的懂的字太多，常常在不同階段針對同樣的概念使用不同的同意詞，反而讓讀者容易眼花。
> - 對於專業學術文章以及回應文字的撰寫，大型語言模型仍然無法掌握一些「眉角」。例如時態的選擇，也許過去式和完成式都是正確的，但背後藏著作者想表達的一些語氣，語言模型並不考慮這一點。